from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
from deepxde.backend import tf
import tensorflow as tf
from tensorflow.keras.optimizers import Optimizer
from scipy.special import gamma
import deepxde as dde
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable

# 使用新的优化器
h = 0.00001
k = 5
ordered = 0.7
a = np.pi
ec = 20000


def sol(x):  # 解析解
    x, t = x[:, 0:1], x[:, 1:2]
    val = np.sin(a * x)
    return np.exp(-a**2 * t) * val


def icfunc(x):  # 初始条件
    return (
        tf.sin(a * x)
    )


def du_t(x):
    x_in, t_in = x[:, 0:1], x[:, 1:2]
    val = np.sin(a * x_in)
    return -a**2 * np.exp(-a**2 * t_in) * val


def du_x(x):
    x_in, t_in = x[:, 0:1], x[:, 1:2]
    val = np.cos(a * x_in)
    return a * np.exp(-a**2 * t_in) * val


def output_transform(x, y):
    x_in = x[:, 0:1]
    t_in = x[:, 1:2]
    return x_in * (x_in-1) * (1-tf.exp(-a**2 * t_in)) * y + icfunc(x_in)


# cdPINN
def cdPINNpde(x, y):
    x_in = x[:, 0:1]
    t_in = x[:, 1:2]
    dy_t = dde.grad.jacobian(y, x, j=1)
    dy_xx = dde.grad.hessian(y, x, i=0, j=0)
    return [dy_t-dy_xx]


class CentralDifferenceAdam(tf.keras.optimizers.Optimizer):
    def __init__(self, learning_rate=0.0001, beta_1=0.99, beta_2=0.999, epsilon=1e-8, h=h, **kwargs):
        super(CentralDifferenceAdam, self).__init__(name="CentralDifferenceAdam", **kwargs)
        self._set_hyper("learning_rate", learning_rate)
        self._set_hyper("beta_1", beta_1)
        self._set_hyper("beta_2", beta_2)
        self._set_hyper("epsilon", epsilon)
        self._set_hyper("h", h)

    def _create_slots(self, var_list):
        for var in var_list:
            self.add_slot(var, "m")
            self.add_slot(var, "v")

    def loss(self, x):
        u_pred = net(x)
        dy_t = dde.grad.jacobian(u_pred, x, j=1)
        dy_xx = dde.grad.hessian(u_pred, x, i=0, j=0)
        return [dy_t-dy_xx]

    def grad_approx(self, loss, var, h):
        var_plus_h = var + h
        var_minus_h = var-h
        loss_plus = loss(var_plus_h)
        loss_minus = loss(var_minus_h)
        grad_approx = (loss_plus-loss_minus) / (2 * h)
        return grad_approx

    def _resource_apply_dense(self, grad_approx, var):
        m = self.get_slot(var, "m")
        v = self.get_slot(var, "v")
        beta_1_t = self._get_hyper("beta_1", tf.float32)
        beta_2_t = self._get_hyper("beta_2", tf.float32)
        lr_t = self._get_hyper("learning_rate", tf.float32)
        epsilon_t = self._get_hyper("epsilon", tf.float32)
        local_step = tf.cast(self.iterations + 1, tf.float32)

        m_t = tf.compat.v1.assign(m, beta_1_t * m + (1.-beta_1_t) * grad_approx, use_locking=self._use_locking)
        v_t = tf.compat.v1.assign(v, beta_2_t * v + (1.-beta_2_t) * tf.square(grad_approx), use_locking=self._use_locking)

        m_hat = m_t / (1.-tf.pow(beta_1_t, local_step))
        v_hat = v_t / (1.-tf.pow(beta_2_t, local_step))

        var_update = tf.compat.v1.assign_sub(var, lr_t * m_hat / (tf.sqrt(v_hat) + epsilon_t), use_locking=self._use_locking)
        return tf.group(*[var_update, m_t, v_t])

    def get_config(self):
        config = super(CentralDifferenceAdam, self).get_config()
        config.update({
            "learning_rate": self._serialize_hyperparameter("learning_rate"),
            "beta_1": self._serialize_hyperparameter("beta_1"),
            "beta_2": self._serialize_hyperparameter("beta_2"),
            "epsilon": self._serialize_hyperparameter("epsilon"),
            "h": self._serialize_hyperparameter("h"),
        })
        return config


geom = dde.geometry.Interval(0, 1)
timedomain = dde.geometry.TimeDomain(0, 1)
geomtime = dde.geometry.GeometryXTime(geom, timedomain)

data = dde.data.TimePDE(
    geomtime,
    cdPINNpde,
    [],
    num_domain=50,
    train_distribution="uniform",
    solution=sol,
    num_test=10000,
)

layer_size = [2] + [20] * 3 + [1]
activation = "tanh"
initializer = "Glorot uniform"
net = dde.maps.FNN(layer_size, activation, initializer)
net.apply_output_transform(output_transform)
cdPINNmodel = dde.Model(data, net)

optimizer = CentralDifferenceAdam(learning_rate=1e-4)
cdPINNmodel.compile(optimizer=optimizer, metrics=["l2 relative error"])
losshistory, train_state = cdPINNmodel.train(epochs=ec, callbacks=[])
dde.saveplot(losshistory, train_state, issave=False, isplot=False)


def gen_test_x(num):
    x = np.linspace(0, 1, num)
    t = np.linspace(1, 0, num)
    l = []

    for i in range(len(t)):
        for j in range(len(x)):
            l.append([x[j], t[i]])
    return np.array(l)


x = gen_test_x(1000)

plt.rcParams.update({"font.size": 50})
X = gen_test_x(1000)
y = cdPINNmodel.predict(x).tolist()
disp = []
prev = X[0][1]
temp = []
for i in range(len(y)):
    if X[i][1] == prev:
        temp.append(y[i][0])
    else:
        prev = X[i][1]
        temp2 = []
        for elem in temp:
            temp2.append((elem))
        disp.append(temp2)
        temp.clear()
        temp.append(y[i][0])
disp.reverse()
plt.figure(figsize=(20, 15))  # 调整图像尺寸
plt.xlabel("x")
plt.ylabel("t")
plt.title("CDadam Prediction")

ax = plt.gca()
im = ax.imshow(disp, extent=(0, 1, 0, 1), origin='lower', aspect='auto')
divider = make_axes_locatable(ax)
width = ax.get_position().width
height = ax.get_position().height
cax = divider.append_axes("right", size="5%", pad=1)
# 绘制等高线图
plt.colorbar(im, cax=cax)
plt.show()

# cdPINN Absolute Error of u
plt.rcParams.update({"font.size": 50})
plt.figure()
X = x
y = cdPINNmodel.predict(x).tolist()
y_true = sol(x)

disp = []
prev = X[0][1]
temp = []

for i in range(len(y)):
    if X[i][1] == prev:
        temp.append(abs(y[i][0]-y_true[i][0]))
    else:
        prev = X[i][1]
        temp2 = []
        for elem in temp:
            temp2.append((elem))
        disp.append(temp2)
        temp.clear()
        temp.append(y[i][0])
disp.reverse()

plt.figure(figsize=(20, 15))
plt.xlabel("x")
plt.ylabel("t")
plt.title("CDadam Error")

ax = plt.gca()

im = ax.imshow(disp, extent=(0, 1, 0, 1), origin='lower', aspect='auto')
divider = make_axes_locatable(ax)
width = ax.get_position().width
height = ax.get_position().height
cax = divider.append_axes("right", size="5%", pad=1)
# 绘制等高线图
plt.colorbar(im, cax=cax)
plt.show()


# results
x = geomtime.uniform_points(1000)

print("L2 relative error of u:")
print("\tcdPINN:", dde.metrics.l2_relative_error(sol(x), cdPINNmodel.predict(x)))

gPINNresiduals = cdPINNmodel.predict(x, operator=cdPINNpde)[0]

print("Mean absolute PDE residual:")
print("\tcdPINN:", np.mean(abs(gPINNresiduals)))

print("L2 relative error of du_x")
print(
    "\tcdPINN:",
    dde.metrics.l2_relative_error(
        du_x(x), cdPINNmodel.predict(x, lambda x, y: dde.grad.jacobian(y, x, j=0))
    ),
)

print("L2 relative error of du_t")
print(
    "\tcdPINN:",
    dde.metrics.l2_relative_error(
        du_t(x), cdPINNmodel.predict(x, lambda x, y: dde.grad.jacobian(y, x, j=1))
    ),
)


# 定义函数
def f(x, t):
    return np.exp(-a**2 * t) * np.sin(a * x)


# 定义x和t的范围
x = np.linspace(0, 1, 400)
t = np.linspace(0, 1, 400)

# 创建网格
X, Y = np.meshgrid(x, t)
# 计算函数值
Z = f(X, Y)
plt.rcParams.update({"font.size": 70})
plt.figure(figsize=(20, 15))
# 绘制等高线图
im = plt.imshow(Z, origin='lower', extent=(0, 1, 0, 1), aspect='auto')
# 创建颜色条
cbar = plt.colorbar(im, orientation='vertical')
# 设置颜色条的刻度格式为3位小数
cbar.formatter = plt.FuncFormatter(lambda x, _: f"{x:.2f}")

# 应用刻度格式器
cbar.update_ticks()
# 设置图表标题和轴标签
plt.title('u(x, t) ')
plt.xlabel('x')
plt.ylabel('t')
# 显示图像
plt.show()
