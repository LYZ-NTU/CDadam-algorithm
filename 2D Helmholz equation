from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
from deepxde.backend import tf
import tensorflow as tf
from tensorflow.keras.optimizers import Optimizer
from scipy.special import gamma
import deepxde as dde
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
from math import *

# 参数设置
h = 0.00001
k = 5
ordered = 0.7
a = np.pi
ec = 20000

# 源项定义
def q(x):
    x_in, y_in = x[:, 0:1], x[:, 1:2]
    q = -tf.sin(a * x_in) * tf.sin(2*a * y_in) * (5*a**2 - 1)
    return q

# 偏导数解析解
def du_x(x):
    x_in, y_in = x[:, 0:1], x[:, 1:2]
    val = a * np.cos(a * x_in) * np.sin(2 * a * y_in)
    return val

def du_y(x):
    x_in, y_in = x[:, 0:1], x[:, 1:2]
    val = 2 * a * np.sin(a * x_in) * np.cos(2 * a * y_in)
    return val

# cdPINN的PDE定义
def cdPINNpde(x, y):
    x_in = x[:, 0:1]
    y_in = x[:, 1:2]
    du_xx = dde.grad.hessian(y, x, i=0, j=0)
    du_yy = dde.grad.hessian(y, x, i=1, j=1)
    return du_xx + du_yy + y - q(x)

# 几何定义
geom = dde.geometry.Rectangle([-1, -1], [1, 1])
data = dde.data.PDE(geom, cdPINNpde, [], num_domain=500)

# 神经网络定义
net = dde.maps.FNN([2] + [20] * 3 + [1], "tanh", "Glorot normal")

# 输出变换
def output_transform(x, y):
    x_in = x[:, 0:1]
    y_in = x[:, 1:2]
    return (1 + x_in) * (1 + y_in) * (1 - x_in) * (1 - y_in) * y

net.apply_output_transform(output_transform)

# 自定义中心差分Adam优化器
class CentralDifferenceAdam(tf.keras.optimizers.Optimizer):
    def __init__(self, learning_rate=0.001, beta_1=0.99, beta_2=0.999, epsilon=1e-8, h=h, **kwargs):
        super(CentralDifferenceAdam, self).__init__(name="CentralDifferenceAdam",** kwargs)
        self._set_hyper("learning_rate", learning_rate)
        self._set_hyper("beta_1", beta_1)
        self._set_hyper("beta_2", beta_2)
        self._set_hyper("epsilon", epsilon)
        self._set_hyper("h", h)

    def _create_slots(self, var_list):
        for var in var_list:
            self.add_slot(var, "m")
            self.add_slot(var, "v")

    def grad_approx(self, loss, var, h):
        var_plus_h = var + h
        var_minus_h = var - h
        loss_plus = loss(var_plus_h)
        loss_minus = loss(var_minus_h)
        grad_approx = (loss_plus - loss_minus) / (2 * h)
        return grad_approx

    def _resource_apply_dense(self, grad_approx, var):
        m = self.get_slot(var, "m")
        v = self.get_slot(var, "v")
        beta_1_t = self._get_hyper("beta_1", tf.float32)
        beta_2_t = self._get_hyper("beta_2", tf.float32)
        lr_t = self._get_hyper("learning_rate", tf.float32)
        epsilon_t = self._get_hyper("epsilon", tf.float32)
        local_step = tf.cast(self.iterations + 1, tf.float32)

        m_t = tf.compat.v1.assign(m, beta_1_t * m + (1. - beta_1_t) * grad_approx, use_locking=self._use_locking)
        v_t = tf.compat.v1.assign(v, beta_2_t * v + (1. - beta_2_t) * tf.square(grad_approx), use_locking=self._use_locking)

        m_hat = m_t / (1. - tf.pow(beta_1_t, local_step))
        v_hat = v_t / (1. - tf.pow(beta_2_t, local_step))

        var_update = tf.compat.v1.assign_sub(var, lr_t * m_hat / (tf.sqrt(v_hat) + epsilon_t), use_locking=self._use_locking)
        return tf.group(*[var_update, m_t, v_t])

    def get_config(self):
        config = super(CentralDifferenceAdam, self).get_config()
        config.update({
            "learning_rate": self._serialize_hyperparameter("learning_rate"),
            "beta_1": self._serialize_hyperparameter("beta_1"),
            "beta_2": self._serialize_hyperparameter("beta_2"),
            "epsilon": self._serialize_hyperparameter("epsilon"),
            "h": self._serialize_hyperparameter("h"),
        })
        return config

# 模型编译与训练
cdPINNmodel = dde.Model(data, net)
optimizer = CentralDifferenceAdam(learning_rate=1e-3)
cdPINNmodel.compile(optimizer=optimizer)
losshistory, train_state = cdPINNmodel.train(epochs=ec, callbacks=[])
dde.saveplot(losshistory, train_state, issave=False, isplot=False)

# 测试数据生成
def gen_test_x(num):
    x = np.linspace(-1, 1, num)
    y = np.linspace(-1, 1, num)
    l = []
    for i in range(len(y)):
        for j in range(len(x)):
            l.append([x[j], y[i]])
    return np.array(l)

# 解析解
def sol(t):
    x = t[:, 0:1]
    y = t[:, 1:2]
    return np.sin(a * x) * np.sin(2 * a * y)

# 预测结果可视化
x = gen_test_x(100)
plt.rcParams.update({"font.size": 30})

plt.figure()
X = gen_test_x(100)
y = cdPINNmodel.predict(x).tolist()

disp = []
prev = X[0][1]
temp = []
for i in range(len(y)):
    if X[i][1] == prev:
        temp.append(y[i][0])
    else:
        prev = X[i][1]
        temp2 = []
        for elem in temp:
            temp2.append(elem)
        disp.append(temp2)
        temp.clear()
        temp.append(y[i][0])
disp.reverse()

plt.figure(figsize=(20, 20))
plt.xlabel("x")
plt.ylabel("y")
plt.title("CDadam Prediction")

ax = plt.gca()
im = ax.imshow(disp, extent=(-1, 1, -1, 1))
ax.set_aspect(1)

divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.2)
plt.colorbar(im, cax=cax)
plt.show()

# 误差可视化
plt.rcParams.update({"font.size": 30})
plt.figure()
X = x
y = cdPINNmodel.predict(x).tolist()
y_true = sol(x)

disp = []
prev = X[0][1]
temp = []
for i in range(len(y)):
    if X[i][1] == prev:
        temp.append(abs(y[i][0] - y_true[i][0]))
    else:
        prev = X[i][1]
        temp2 = []
        for elem in temp:
            temp2.append(elem)
        disp.append(temp2)
        temp.clear()
        temp.append(y[i][0])
disp.reverse()

plt.figure(figsize=(20, 20))
plt.xlabel("x")
plt.ylabel("y")
plt.title("CDadam Error")

ax = plt.gca()
im = ax.imshow(disp, extent=(-1, 1, -1, 1))
ax.set_aspect(1)

divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.2)
plt.colorbar(im, cax=cax)
plt.show()

# Adam优化器的PINN模型（保留用于对比）
def PINNpde(x, y):
    x_in = x[:, 0:1]
    y_in = x[:, 1:2]
    du_xx = dde.grad.hessian(y, x, i=0, j=0)
    du_yy = dde.grad.hessian(y, x, i=1, j=1)
    return du_xx + du_yy + y - q(x)

geom = dde.geometry.Rectangle([-1, -1], [1, 1])
data = dde.data.PDE(geom, PINNpde, [], num_domain=500)
net = dde.maps.FNN([2] + [20] * 3 + [1], "tanh", "Glorot normal")

def output_transform(x, y):
    x_in = x[:, 0:1]
    y_in = x[:, 1:2]
    return (1 + x_in) * (1 + y_in) * (1 - x_in) * (1 - y_in) * y

net.apply_output_transform(output_transform)

PINNmodel = dde.Model(data, net)
PINNmodel.compile("adam", lr=1e-3)
losshistory, train_state = PINNmodel.train(epochs=ec, callbacks=[])

# Adam模型结果可视化（保留用于对比）
x = gen_test_x(100)
plt.rcParams.update({"font.size": 30})

plt.figure()
X = gen_test_x(100)
y = PINNmodel.predict(x).tolist()

disp = []
prev = X[0][1]
temp = []
for i in range(len(y)):
    if X[i][1] == prev:
        temp.append(y[i][0])
    else:
        prev = X[i][1]
        temp2 = []
        for elem in temp:
            temp2.append(elem)
        disp.append(temp2)
        temp.clear()
        temp.append(y[i][0])
disp.reverse()

plt.figure(figsize=(20, 20))
plt.xlabel("x")
plt.ylabel("y")
plt.title("Adam Prediction")

ax = plt.gca()
im = ax.imshow(disp, extent=(-1, 1, -1, 1))
ax.set_aspect(1)

divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.2)
plt.colorbar(im, cax=cax)
plt.show()

# Adam模型误差可视化（保留用于对比）
plt.rcParams.update({"font.size": 30})
plt.figure()
X = x
y = PINNmodel.predict(x).tolist()
y_true = sol(x)

disp = []
prev = X[0][1]
temp = []
for i in range(len(y)):
    if X[i][1] == prev:
        temp.append(abs(y[i][0] - y_true[i][0]))
    else:
        prev = X[i][1]
        temp2 = []
        for elem in temp:
            temp2.append(elem)
        disp.append(temp2)
        temp.clear()
        temp.append(y[i][0])
disp.reverse()

plt.figure(figsize=(20, 20))
plt.xlabel("x")
plt.ylabel("y")
plt.title("Adam Error")

ax = plt.gca()
im = ax.imshow(disp, extent=(-1, 1, -1, 1))
ax.set_aspect(1)

divider = make_axes_locatable(ax)
cax = divider.append_axes("right", size="5%", pad=0.2)
plt.colorbar(im, cax=cax)
plt.show()

# 结果对比
print("L2 relative error of u:")
print("\tPINN:", dde.metrics.l2_relative_error(sol(x), PINNmodel.predict(x)))
print("\tcdPINN:", dde.metrics.l2_relative_error(sol(x), cdPINNmodel.predict(x)))

PINNresiduals = PINNmodel.predict(x, operator=PINNpde)[0]
cdPINNresiduals = cdPINNmodel.predict(x, operator=cdPINNpde)[0]

print("Mean absolute PDE residual:")
print("\tPINN:", np.mean(abs(PINNresiduals)))
print("\tcdPINN:", np.mean(abs(cdPINNresiduals)))

print("L2 relative error of du_x")
print(
    "\tPINN:",
    dde.metrics.l2_relative_error(
        du_x(x), PINNmodel.predict(x, lambda x, y: dde.grad.jacobian(y, x, j=0))
    ),
)
print(
    "\tcdPINN:",
    dde.metrics.l2_relative_error(
        du_x(x), cdPINNmodel.predict(x, lambda x, y: dde.grad.jacobian(y, x, j=0))
    ),
)

print("L2 relative error of du_y")
print(
    "\tPINN:",
    dde.metrics.l2_relative_error(
        du_y(x), PINNmodel.predict(x, lambda x, y: dde.grad.jacobian(y, x, j=1))
    ),
)
print(
    "\tcdPINN:",
    dde.metrics.l2_relative_error(
        du_y(x), cdPINNmodel.predict(x, lambda x, y: dde.grad.jacobian(y, x, j=1))
    ),
)

# 解析解可视化
a = np.pi
def f(x, y):
    return np.sin(a * x) * np.sin(2 * a * y)

x = np.linspace(-1, 1, 400)
y = np.linspace(-1, 1, 400)
X, Y = np.meshgrid(x, y)
Z = f(X, Y)

plt.rcParams.update({"font.size": 50})
plt.figure(figsize=(15, 15))
contour = plt.contourf(X, Y, Z, levels=14, cmap='viridis')
cbar = plt.colorbar(contour, orientation='vertical')
cbar.formatter = plt.FuncFormatter(lambda x, _: f"{x:.4f}")
cbar.update_ticks()
plt.title('u(x, y) ')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
